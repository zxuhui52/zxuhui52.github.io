{
  "hash": "e09f4ed6820bdf7ee0f124a3f219c25b",
  "result": {
    "markdown": "---\ntitle: Introduction to Linear Regression and Non-Linear Regression\nauthor: Xuhui Zeng\ndate: now\ncategories:\n  - regression\n  - machine learning\n  - code\nformat: html\n---\n\n**Regression** is a statistical method used for modeling the relationship between a dependent variable (target) and one or more independent variables (features). The primary goal of regression analysis is to understand how the independent variables contribute to the variability in the dependent variable. The dependent variable in a regression task is numerical and usually continuous, rather than catergorical in a classification task. Regression is usually categorized into **linear regression** and **non-linear regression**.\n\nLinear Regression assumes a linear relationship between the dependent and independent variables, and models the relationship by fitting a linear equation to the observed data. The simplest linear equation takes the following form:\n\n$Y = mX + b$\n\nwhere:\n\n-   $Y$ is the dependent variable.\n-   $X$ is the independent variable.\n-   $m$ is the slope of the line.\n-   $b$ is the y-intercept.\n\nThe linear equation can be extended to include multiple independent variables as follows:\n\n$Y = b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n$\n\nThis is a multivariate linear regression assuming linear relationship in a higher dimension feature space.\n\nWe can make up a scatter plot with a fitting line to see what a typical linear regression looks like:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data\nX = np.linspace(0, 10, 100)  # Independent variable\nnoise = np.random.normal(0, 2, 100)  # Gaussian noise\ny = 2 * X + 1 + noise  # Linear relationship with noise\n\n# Create a DataFrame\ndata = pd.DataFrame({'X': X, 'y': y})\n\nplt.figure(figsize=(10, 6))\nsns.regplot(x='X', y='y', data=data, line_kws={'color': 'red', 'label': 'Regression Line'})\n\nplt.title('Single-Variable Linear Regression')\nplt.xlabel('Independent Variable (X)')\nplt.ylabel('Dependent Variable (y)')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=808 height=523}\n:::\n:::\n\n\nThe real-word data are usually more complicated, and can not be described by a simple linear equation. The **non-Linear regression** extends regression analysis to model relationships that are not linear. The functional form of the relationship between variables is more complex and can take various shapes. It is suitable for data where the variance of errors is not constant across all levels of the independent variable.\n\nNon-linear regression can involve various mathematical functions like **exponential**, **logarithmic**, **polynomial**, or even **custom functions** based on the nature of the data. This is called a **parametric approach** in which you use one or more mathematical models to fit the data.\n\nPolynomial equation is a typical non-linear regression equation and takes the following form:\n\n$Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon$\n\nwhere:\n\n-   $Y$ is the dependent variable.\n-   $X$ is the independent variable.\n-   $\\beta_0, \\beta_1, \\beta_2$ are parameters.\n-   $\\epsilon$ is the error term.\n\nLet's see what this may look like:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\n# Fit polynomial features\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n\n# Fit linear regression\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\n\n# Plot the data and the fitted curve\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=X.flatten(), y=y.flatten(), label='Data')\nsns.lineplot(x=X.flatten(), y=lin_reg.predict(X_poly).flatten(), color='red', label='Polynomial Regression')\nplt.title('Polynomial Regression Example')\nplt.xlabel('Independent Variable (X)')\nplt.ylabel('Dependent Variable (y)')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=799 height=523}\n:::\n:::\n\n\nThere is another way to do non-linear regression. For example, regression with **decision tree** does not assume a mathematical relationship. Instead, the alrogithm recursively splits the data into subsets based on the values of input features, creating a tree structure. Predictions are made by averaging the target values in the leaf nodes reached by a given input. This is called a **non-parametric approach**. It is well-suited for capturing complex, non-linear relationships as it can approximate arbitrary shapes.\n\nSimilar to decision tree classifier, a decision tree regressor can be prone to overfitting, creating branches tailored to the training data. Pruning techniques are often applied to prevent overfitting. In addition, ensemble methods like Random Forests (bagging) and Gradient Boosting Decision Tree (boosting) are commonly used to enhance performance and reduce overfitting.\n\nLet's see how a decision tree work on a regression task:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=3)\nregr_2 = DecisionTreeRegressor(max_depth=6)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\nX_test = np.arange(0.0, 2.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n\n# Plot the results using Seaborn for styling\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=X.flatten(), y=y.flatten(), label=\"data\")\nsns.lineplot(x=X_test.flatten(), y=y_1.flatten(), label=\"max_depth=3\", color=\"red\", linewidth=2)\nsns.lineplot(x=X_test.flatten(), y=y_2.flatten(), label=\"max_depth=6\", color=\"yellowgreen\", linewidth=2)\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"Decision Tree Regression Example\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=799 height=523}\n:::\n:::\n\n\nWe can see that if the maximum depth of the tree (controlled by the `max_depth` parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit.\n\nThe choice between Polynomial Regression and Decision Tree Regression depends on the specific characteristics of the dataset and the nature of the relationship being modeled. Polynomial regression is suitable when the underlying relationship is polynomial, while decision tree regression provides flexibility for capturing non-linear and complex patterns without specifying a particular function form. The decision often involves trade-offs between interpretability, computational efficiency, and the ability to handle different types of relationships.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}