{
  "hash": "06091c82e279fdb54229085a01a3c81f",
  "result": {
    "markdown": "---\ntitle: Theory behind Naive Bayes Classifier\nauthor: Xuhui Zeng\ndate: now\ncategories:\n  - probability theory\n  - machine learning\n  - code\nformat: html\n---\n\nInteresting fact that Naive Bayes is different from the majority of other classification algorithms. Most of the classification algorithms, such as decision trees, KNN, logistic regression, support vector machines, etc., they are **discriminative** methods.at and th e output target $Y = f()X$ or a conditional distribution $P(Y|X)$. However, Naive Bayes is a **generative** method, meaning it directly determines the joint distribution $P(X,Y)$ between the features X and target Y. Then it uses $P(Y|X) = P(X,Y)/P(X)$ to derive the results.\n\nNaive Bayes is based on **Bayes' Theorem**. It is called \"naive\" because it makes the assumption that the features used to describe an observation are conditionally independent, given the class label. This assumption simplifies the computation and is where the term \"naive\" comes from. Being \"naive\" does not mean it does not produce a good performance. In fact, Naive Bayesis particularly effective when dealing with categorical data, such as text classification tasks, where the features represent word occurrences or frequencies. It is commonly used in natural language processing (NLP) applications like spam filtering and sentiment analysis.\n\nTo understand how a Naive Bayes classifier works, we need to start with the Bayes' Theorem. It is a fundamental principle in probability theory that describes the probability of an event based on prior knowledge of conditions that might be related to the event, also known as **conditional probability**. It is named after the Reverend Thomas Bayes, an 18th-century statistician and theologian. The formula for Bayes' Theorem is as follows:\n\n$P(A│B)=\\frac{P(A)P(B|A)}{P(B)}$\n\nWhere:\n\n-   $P(A│B)$ is the probability of event A occurring given that event B has occurred.\n-   $P(B│A)$ is the probability of event B occurring given that event A has occurred.\n-   $P(A)$ is the prior probability of event A.\n-   $P(B)$ is the prior probability of event B.\n\nThe general form of the Naive Bayes classifier for a class $C_k$ and features $x_1, x_2, ..., x_n$ is:\n\n$P(C_k | x_1, x_2, ..., x_n) = \\frac{P(x_1) \\cdot P(x_2) \\cdot ... \\cdot P(x_n) \\cdot P(C_k)}{P(x_1 | C_k) \\cdot P(x_2 | C_k) \\cdot ... \\cdot P(x_n | C_k)}​$\n\nThe key assumption here is that the features $x_1, x_2, ..., x_n$ are conditionally independent given the class label $C_k$.\n\nThe key assumption here is that the features $x_1, x_2, ..., x_n$ are conditionally independent given the class label $C_k$. The prior probability of each class $C_k$ can be calculated by counting the occurrences of each class and dividing by the total number of examples. The conditional probability $P(x_i|C_k)$ can be calculated by counting the occurrences of $x_i$ within samples labeled as $C_k$ and divided by the total number of samples labeled as $C_k$.\n\nTo make a predition for a new unlabeled sample with features $X$, the classifier first calculate the posterior probability for each class $C_k$ using the Bayes' Theorem:\n\n$P(C_k | X) = \\frac{P(X | C_k) \\cdot P(C_k)}{P(X)}$\n\nThen it makes a decision by assigning class label $Y$ that maximizes the posterior probability $P(C_k | X)$:\n\n$Y = \\arg\\max_{C_k} P(C_k | X)$\n\nLet's see how a Naive Bayes classifier works on the breast cancer dataset:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a Gaussian Naive Bayes classifier\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = nb_classifier.predict(X_test)\n\n# Generate a confusion matrix and classification report\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=data.target_names, yticklabels=data.target_names)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Display the classification report\nprint(\"Classification Report:\\n\", classification_rep)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=655 height=523}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      0.93      0.96        43\n           1       0.96      1.00      0.98        71\n\n    accuracy                           0.97       114\n   macro avg       0.98      0.97      0.97       114\nweighted avg       0.97      0.97      0.97       114\n\n```\n:::\n:::\n\n\nThe Naive Bayes classifier using GaussianNB (assuming prior probability follows a normal distribution) can do an excellent job in this classification task with high precision and recall at the same time.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}