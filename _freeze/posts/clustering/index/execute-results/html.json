{
  "hash": "6aedb15f348f381014e863e438bbba3e",
  "result": {
    "markdown": "---\ntitle: Introduction to Clustering with DBSCAN\nauthor: Xuhui Zeng\ndate: now\ncategories:\n  - clustering\n  - machine learning\n  - code\nformat: html\n---\n\nTraditioal K-means clustering is well-suited for certain types of datasets and scenarios. It tends to perform well on datasets in which the clusters have a roughly circular shape in the feature space, similar variances, and approximately equal sizes. However, since K-means assumes that clusters are spherical and equally sized, there are a lot of situations that K-means cannot handle. For example, if the clusters have **irregular shapes**, **varying densities**, or the clusters are **not convex**, K-means will have poor performance on the datasets. As a result, alternative clustering algorithms may be considered. **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) is a great candicate to handle such complex datasets.\n\nDBSCAN is a clustering algorithm designed to identify clusters in a dataset based on the **density** of data points. Unlike K-means, which assumes that clusters are spherical and equally sized, DBSCAN can discover clusters with irregular shapes and handle varying cluster densities.\n\nThe key idea behind DBSCAN is to group together data points that are close to each other in high-density regions and separate regions of lower density. It works based on the following parameters:\n\n-   **Epsilon** (`eps`)\n-   **Minimum Points** (`min_samples`)\n\nEpsilon defines the radius around a data point within which the algorithm looks for neighboring points. It is a key parameter that influences the size of the neighborhood considered for density estimation.\n\nMinimum Points specifies the minimum number of data points required to form a dense region. If a point has at least MinPts points within its epsilon neighborhood, it is considered a core point. Points that have fewer neighbors than MinPts but fall within the epsilon neighborhood of a core point are considered border points, while those with no neighbors are treated as noise or outliers.\n\nThe DBSCAN algorithm proceeds as follows:\n\n-   Identify core points, which are data points with at least MinPts neighbors within a radius of eps.\n-   Form clusters by connecting core points that are within each other's epsilon neighborhood.\n-   Expand the clusters by adding border points to them. Border points are points that have fewer than MinPts neighbors but are within the epsilon neighborhood of a core point.\n-   Identify noise points as data points that do not belong to any cluster.\n\nThe following example shows how DBSCAN can perform better on a non-convex dataset than K-means, and how we can adjust `eps` and `min_samples` parameters to achieve the goal of clustering. First, let's generate a synthetic data set with 3 clusters, 2 of which are non-convex.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\n\n# Generate synthetic datasets\nX1, y1 = datasets.make_circles(n_samples=5000, factor=.6, noise=.05)\nX2, y2 = datasets.make_blobs(n_samples=1000, n_features=2, centers=[[1.2, 1.2]], cluster_std=[[.1]], random_state=42)\nX = np.concatenate((X1, X2))\n\n# Create a scatter plot\nsns.set(style=\"white\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X[:, 0], y=X[:, 1], marker='o', alpha=0.7)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=656 height=489}\n:::\n:::\n\n\nLet's see how K-means work on the dataset. The `n_clusters` is set to 3 knowing that there are 3 clusters in the dataset.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\n\n# Perform KMeans clustering\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\ny_pred = kmeans.fit_predict(X)\n\n# Create a scatter plot\nsns.set(style=\"white\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred, palette='viridis', legend=False, marker='o', alpha=0.7)\n\nplt.show();\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=656 height=489}\n:::\n:::\n\n\nApparently, K-means can not identify the two non-convex clusters. Let's set up a DBSCAN to see how it performs.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.cluster import DBSCAN\n\n# Perform DBSCAN clustering\ndbscan = DBSCAN(eps=0.2, min_samples=5)\ny_pred = dbscan.fit_predict(X)\n\n# Create a scatter plot\nsns.set(style=\"white\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred, palette='viridis', legend=False, marker='o', alpha=0.7)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=656 height=489}\n:::\n:::\n\n\nThe above DBSCAN algorithm sets the `eps` at 0.2 and `min_samples` at 0.5. It can differentiate between the convex cluster and the non-convex clusters, which a sign of improvement. However, it is not differentiating the two non-convex clusters, which could mean that the size of the dense region may be too large. We can either decrease the `eps` or `min_samples` to shrink the dense region.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Update hyperparameters for DBSCAN clustering\ndbscan = DBSCAN(eps=0.1, min_samples=5)\ny_pred = dbscan.fit_predict(X)\n\n# Create a scatter plot\nsns.set(style=\"white\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred, palette='viridis', legend=False, marker='o', alpha=0.7)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=656 height=489}\n:::\n:::\n\n\nBy decreasing the `eps` to 0.1, the DBSCAN alrogithm now does an excellent job in differentiating the two non-convex clusters.\n\nThe example demonstrate DBSCAN's ability to handle clusters of arbitrary shapes, insensitivity to the order of the input data, and ability to discover clusters of different shapes and sizes. Additionally, DBSCAN can automatically determine the number of clusters in the dataset. However, DBSCAN's performance can be sensitive to the choice of parameters, and it may struggle with datasets of varying densities. Tuning these parameters appropriately for the specific dataset is crucial for obtaining meaningful clustering results.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}