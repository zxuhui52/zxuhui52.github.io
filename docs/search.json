[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "zxuhui52.github.io",
    "section": "",
    "text": "Classification with Decision Tree and Random Forest\n\n\n\n\n\n\n\nclassification\n\n\nmachine learning\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nXuhui Zeng\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Clustering with DBSCAN\n\n\n\n\n\n\n\nclustering\n\n\nmachine learning\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nXuhui Zeng\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Linear Regression and Non-Linear Regression\n\n\n\n\n\n\n\nregression\n\n\nmachine learning\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nXuhui Zeng\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "docs/posts/classification/index.html",
    "href": "docs/posts/classification/index.html",
    "title": "Classification with Decision Tree and Random Forest",
    "section": "",
    "text": "A decision tree is a hierarchical tree-like structure composed of a root node, internal nodes, and leaf nodes. In the context of classification, each leaf node represents a specific class or decision outcome, while the internal nodes correspond to attribute tests. The tree structure is built by recursively splitting the dataset based on the values of different attributes.\nThe process starts at the root node, which contains the entire dataset, and, through a series of attribute tests, the data is partitioned into subsets as it traverses down the tree. This recursive process continues until reaching the leaf nodes, where the final classification decision is made. Essentially, a decision tree works by iteratively asking if/else questions about the input features to make a sequence of decisions that lead to the classification of instances into specific classes at the leaf nodes.\nDecision trees are susceptible to overfitting when allowed to grow until all nodes become pure leaf nodes. Overfitting occurs when the model fits the training data too closely, capturing noise and fluctuations in the data, which may not generalize well to new, unseen data.\nTo address the overfitting challenge in decision trees, strategies are employed to prevent them from becoming overly complex:\nPre-pruning (Early Stopping): This strategy involves stopping the tree-building process prematurely. Limiting the number of leaf nodes or the maximum depth of the tree helps control its complexity. By setting these constraints, the tree is prevented from becoming too deep and capturing noise in the training data.\nPost-pruning (Pruning after Construction): In this approach, the tree is fully constructed, and then some branches are either removed or combined. The decision to prune branches is based on the information content they provide. If a branch contributes little to the overall predictive power of the tree, it may be pruned to simplify the model and improve generalization to new data.\nLet’s implement a decision tree classification on the breast cancer dataset without performing pre-pruning, allowing the tree to grow until every node becomes a leaf node.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\n# Load the breast cancer dataset\ncancer_data = load_breast_cancer()\nfeatures = cancer_data.data\nlabels = cancer_data.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, labels, stratify=labels, random_state=42)\n\n# Initialize and fit a Decision Tree Classifier\ntree_classifier_1 = DecisionTreeClassifier(random_state=42)\ntree_classifier_1.fit(X_train, y_train)\n\n# Evaluate the model on the training set\ntraining_accuracy = tree_classifier_1.score(X_train, y_train)\nprint(\"Training set accuracy: {:.4f}\".format(training_accuracy))\n\n# Evaluate the model on the test set\ntest_accuracy = tree_classifier_1.score(X_test, y_test)\nprint(\"Test set accuracy: {:.4f}\".format(test_accuracy))\n\n# Get the depth of the tree\nprint(\"Depth of the decision tree: {}\".format(tree_classifier_1.get_depth()))\n\nTraining set accuracy: 1.0000\nTest set accuracy: 0.9231\nDepth of the decision tree: 7\n\n\nThe decision tree has grown to a deep tree with a depth of 7. As a result, the accuracy on the training set has 100%, indicating every node becomes a leaf node. On the other hand, the accuracy of the test set is lower at 92%. While it is still very high, it may be a sign of overfitting that reduces that generalization ability of the model. To mitigate overfitting, we can use the pre-pruning technique to limit the depth of the tree by setting the max_depth parameter.\n\n# Initialize and fit a new Decision Tree Classifier, and limit the max_depth to 4\ntree_classifier_2 = DecisionTreeClassifier(random_state=42, max_depth=4)\ntree_classifier_2.fit(X_train, y_train)\n\n# Evaluate the model on the training set\ntraining_accuracy = tree_classifier_2.score(X_train, y_train)\nprint(\"Training set accuracy: {:.4f}\".format(training_accuracy))\n\n# Evaluate the model on the test set\ntest_accuracy = tree_classifier_2.score(X_test, y_test)\nprint(\"Test set accuracy: {:.4f}\".format(test_accuracy))\n\nTraining set accuracy: 0.9883\nTest set accuracy: 0.9441\n\n\nBy limiting the maximum depth of the decision tree, although the accuracy on the training set decreases, the model demonstrates improved generalization on new data. This reflects enhanced capability to perform well on unseen instances.\nLet’s see what this pruned tree looks like:\n\nfrom sklearn.tree import plot_tree\n\nplt.rcParams['figure.dpi']=300\nplot_tree(\n    tree_classifier_2,\n    filled=True,\n    rounded=True,\n    feature_names=list(cancer_data.feature_names),\n    class_names=[\"m\",\"b\"],\n    fontsize=3,\n);\n\n\n\n\nWe can see there are only 11 benign samples on the entire left tree to the root node (worst radius &lt;= 16.795), this is showing that this feature has a very high weight in the model, which may still indicate a risk of overfitting. The classifier relies heavily on just a few of the features. Essentially, in lieu of reviewing the entire tree, we can just visulize the importance of features by order.\n\ndef plot_feature_importances_cancer(model):\n    feature_importances = model.feature_importances_\n\n    sns.set(rc={\"axes.labelsize\": 10, \"xtick.labelsize\": 8, \"ytick.labelsize\": 8})\n    \n    # Create a DataFrame for better integration with Seaborn\n    data = pd.DataFrame({'Feature': cancer_data.feature_names, 'Importance': feature_importances})\n    \n    # Plot using Seaborn barplot\n    sns.barplot(x='Importance', hue='Feature', legend=False, data=data, palette='viridis')\n    \n    plt.xlabel(\"Feature Importances\")\n    plt.ylabel(\"Feature Name\")\n    plt.title(\"Feature Importances in Cancer Dataset\")\n    plt.show()\n\n\nplt.rcParams['figure.dpi']=120\nplot_feature_importances_cancer(tree_classifier_2)\n\n\n\n\nThe advantages of a decision tree include:\n- Interpretability: Decision trees are easy to understand and interpret, making them suitable for explaining the reasoning behind decisions.\n- No Assumptions about Data: Decision trees do not make assumptions about the distribution of data and can handle both numerical and categorical features.\n- Feature Selection: Decision trees implicitly perform feature selection by identifying the most informative features at each split.\nOn the other hand, the disadvantages of a decision tree include:\n- Overfitting: Decision trees are prone to overfitting, especially when they are deep and too complex. This can lead to poor generalization on new, unseen data.\n- Instability: Small variations in the data can result in a completely different tree structure, making decision trees somewhat unstable.\nOne way to overcome the disadvantages of a decision tree is to use an ensemble of decision trees. The idea behind ensembles is based on the concept that combining diverse models can compensate for their individual weaknesses, leading to better overall performance.\nRandom Forest is an ensemble method that operates by constructing a multitude of decision trees at training time and outputs the class that is the mode of the classes in a classification task. It builds multiple decision trees and merges them together to get a more accurate and stable prediction. The figure below shows the general idea of a random forest.\n\nTo construct a random forest classifier, the number of trees to be built needs to be determined, which is set using the n_estimators parameter.\nFor building each individual tree, the process begins with bootstrapping. Bootstrapping involves randomly sampling with replacement from the pool of n_samples data points, creating a new dataset of the same size. This means that the same data point may appear multiple times in the new dataset. Subsequently, a decision tree is constructed based on this bootstrapped dataset.\nDuring the construction of each tree, a random subset of features is chosen for testing, instead of considering all features. The number of features in the subset is controlled by the max_features parameter. This combination of bootstrapping and testing with random feature subsets ensures that each tree in the random forest is different.\nFor regression problems, the final result is the average of the results from all trees in the random forest. In classification problems, a soft voting approach is used, where the average probabilities for different classes from each tree are calculated, and the class with the highest average probability is chosen as the final result.\nLet’s try constructing a random forest classifier with 100 decision trees on the same breast cancer dataset:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize and fit a Random Forest Classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Evaluate the model on the training set\ntraining_accuracy = rf_classifier.score(X_train, y_train)\nprint(\"Training set accuracy: {:.4f}\".format(training_accuracy))\n\n# Evaluate the model on the test set\ntest_accuracy = rf_classifier.score(X_test, y_test)\nprint(\"Test set accuracy: {:.4f}\".format(test_accuracy))\n\nTraining set accuracy: 1.0000\nTest set accuracy: 0.9580\n\n\nWe can see that the accuracy of the random forest is 96%, which is better than the results obtained from individual decision trees even after pruning. It’s possible to fine-tune the max_features parameter or perform pre-pruning on the trees within the forest. However, in many cases, the default parameters are good enough to produce satisfactory results.\nSimilarly, we can visulize the importance of features by order:\n\nplot_feature_importances_cancer(rf_classifier)\n\n\n\n\nUnlike a single decision tree, many of the feature importances are great than 0 in a random forest. The weights of the features are more averaged out compared to those in a single decision tree.\nThe advantages of a random forest include:\n- Improved Generalization: Random Forest mitigates overfitting by aggregating predictions from multiple trees, leading to better generalization on new data.\n- Robustness: Random Forest is less sensitive to noise and outliers in the data compared to individual decision trees.\n- Feature Importance: Random Forest provides a measure of feature importance, helping in feature selection.\nThe disadvantages of a random forest include:\n- Less Interpretability: While an individual decision tree is easy to interpret, the ensemble nature of Random Forest makes it less straightforward to explain the overall model reasoning.\n- Computational Complexity: Training a Random Forest can be computationally expensive, especially with a large number of trees and features."
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification with Decision Tree and Random Forest",
    "section": "",
    "text": "A decision tree is a hierarchical tree-like structure composed of a root node, internal nodes, and leaf nodes. In the context of classification, each leaf node represents a specific class or decision outcome, while the internal nodes correspond to attribute tests. The tree structure is built by recursively splitting the dataset based on the values of different attributes.\nThe process starts at the root node, which contains the entire dataset, and, through a series of attribute tests, the data is partitioned into subsets as it traverses down the tree. This recursive process continues until reaching the leaf nodes, where the final classification decision is made. Essentially, a decision tree works by iteratively asking if/else questions about the input features to make a sequence of decisions that lead to the classification of instances into specific classes at the leaf nodes.\nDecision trees are susceptible to overfitting when allowed to grow until all nodes become pure leaf nodes. Overfitting occurs when the model fits the training data too closely, capturing noise and fluctuations in the data, which may not generalize well to new, unseen data.\nTo address the overfitting challenge in decision trees, strategies are employed to prevent them from becoming overly complex:\nPre-pruning (Early Stopping): This strategy involves stopping the tree-building process prematurely. Limiting the number of leaf nodes or the maximum depth of the tree helps control its complexity. By setting these constraints, the tree is prevented from becoming too deep and capturing noise in the training data.\nPost-pruning (Pruning after Construction): In this approach, the tree is fully constructed, and then some branches are either removed or combined. The decision to prune branches is based on the information content they provide. If a branch contributes little to the overall predictive power of the tree, it may be pruned to simplify the model and improve generalization to new data.\nLet’s implement a decision tree classification on the breast cancer dataset without performing pre-pruning, allowing the tree to grow until every node becomes a leaf node.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\n# Load the breast cancer dataset\ncancer_data = load_breast_cancer()\nfeatures = cancer_data.data\nlabels = cancer_data.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, labels, stratify=labels, random_state=42)\n\n# Initialize and fit a Decision Tree Classifier\ntree_classifier_1 = DecisionTreeClassifier(random_state=42)\ntree_classifier_1.fit(X_train, y_train)\n\n# Evaluate the model on the training set\ntraining_accuracy = tree_classifier_1.score(X_train, y_train)\nprint(\"Training set accuracy: {:.4f}\".format(training_accuracy))\n\n# Evaluate the model on the test set\ntest_accuracy = tree_classifier_1.score(X_test, y_test)\nprint(\"Test set accuracy: {:.4f}\".format(test_accuracy))\n\n# Get the depth of the tree\nprint(\"Depth of the decision tree: {}\".format(tree_classifier_1.get_depth()))\n\nTraining set accuracy: 1.0000\nTest set accuracy: 0.9231\nDepth of the decision tree: 7\n\n\nThe decision tree has grown to a deep tree with a depth of 7. As a result, the accuracy on the training set has 100%, indicating every node becomes a leaf node. On the other hand, the accuracy of the test set is lower at 92%. While it is still very high, it may be a sign of overfitting that reduces that generalization ability of the model. To mitigate overfitting, we can use the pre-pruning technique to limit the depth of the tree by setting the max_depth parameter.\n\n# Initialize and fit a new Decision Tree Classifier, and limit the max_depth to 4\ntree_classifier_2 = DecisionTreeClassifier(random_state=42, max_depth=4)\ntree_classifier_2.fit(X_train, y_train)\n\n# Evaluate the model on the training set\ntraining_accuracy = tree_classifier_2.score(X_train, y_train)\nprint(\"Training set accuracy: {:.4f}\".format(training_accuracy))\n\n# Evaluate the model on the test set\ntest_accuracy = tree_classifier_2.score(X_test, y_test)\nprint(\"Test set accuracy: {:.4f}\".format(test_accuracy))\n\nTraining set accuracy: 0.9883\nTest set accuracy: 0.9441\n\n\nBy limiting the maximum depth of the decision tree, although the accuracy on the training set decreases, the model demonstrates improved generalization on new data. This reflects enhanced capability to perform well on unseen instances.\nLet’s see what this pruned tree looks like:\n\nfrom sklearn.tree import plot_tree\n\nplt.rcParams['figure.dpi']=300\nplot_tree(\n    tree_classifier_2,\n    filled=True,\n    rounded=True,\n    feature_names=list(cancer_data.feature_names),\n    class_names=[\"m\",\"b\"],\n    fontsize=3,\n);\n\n\n\n\nWe can see there are only 11 benign samples on the entire left tree to the root node (worst radius &lt;= 16.795), this is showing that this feature has a very high weight in the model, which may still indicate a risk of overfitting. The classifier relies heavily on just a few of the features. Essentially, in lieu of reviewing the entire tree, we can just visulize the importance of features by order.\n\ndef plot_feature_importances_cancer(model):\n    feature_importances = model.feature_importances_\n\n    sns.set(rc={\"axes.labelsize\": 10, \"xtick.labelsize\": 8, \"ytick.labelsize\": 8})\n    \n    # Create a DataFrame for better integration with Seaborn\n    data = pd.DataFrame({'Feature': cancer_data.feature_names, 'Importance': feature_importances})\n    \n    # Plot using Seaborn barplot\n    sns.barplot(x='Importance', hue='Feature', legend=False, data=data, palette='viridis')\n    \n    plt.xlabel(\"Feature Importances\")\n    plt.ylabel(\"Feature Name\")\n    plt.title(\"Feature Importances in Cancer Dataset\")\n    plt.show()\n\n\nplt.rcParams['figure.dpi']=120\nplot_feature_importances_cancer(tree_classifier_2)\n\n\n\n\nThe advantages of a decision tree include:\n- Interpretability: Decision trees are easy to understand and interpret, making them suitable for explaining the reasoning behind decisions.\n- No Assumptions about Data: Decision trees do not make assumptions about the distribution of data and can handle both numerical and categorical features.\n- Feature Selection: Decision trees implicitly perform feature selection by identifying the most informative features at each split.\nOn the other hand, the disadvantages of a decision tree include:\n- Overfitting: Decision trees are prone to overfitting, especially when they are deep and too complex. This can lead to poor generalization on new, unseen data.\n- Instability: Small variations in the data can result in a completely different tree structure, making decision trees somewhat unstable.\nOne way to overcome the disadvantages of a decision tree is to use an ensemble of decision trees. The idea behind ensembles is based on the concept that combining diverse models can compensate for their individual weaknesses, leading to better overall performance.\nRandom Forest is an ensemble method that operates by constructing a multitude of decision trees at training time and outputs the class that is the mode of the classes in a classification task. It builds multiple decision trees and merges them together to get a more accurate and stable prediction. The figure below shows the general idea of a random forest.\n\nTo construct a random forest classifier, the number of trees to be built needs to be determined, which is set using the n_estimators parameter.\nFor building each individual tree, the process begins with bootstrapping. Bootstrapping involves randomly sampling with replacement from the pool of n_samples data points, creating a new dataset of the same size. This means that the same data point may appear multiple times in the new dataset. Subsequently, a decision tree is constructed based on this bootstrapped dataset.\nDuring the construction of each tree, a random subset of features is chosen for testing, instead of considering all features. The number of features in the subset is controlled by the max_features parameter. This combination of bootstrapping and testing with random feature subsets ensures that each tree in the random forest is different.\nFor regression problems, the final result is the average of the results from all trees in the random forest. In classification problems, a soft voting approach is used, where the average probabilities for different classes from each tree are calculated, and the class with the highest average probability is chosen as the final result.\nLet’s try constructing a random forest classifier with 100 decision trees on the same breast cancer dataset:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize and fit a Random Forest Classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Evaluate the model on the training set\ntraining_accuracy = rf_classifier.score(X_train, y_train)\nprint(\"Training set accuracy: {:.4f}\".format(training_accuracy))\n\n# Evaluate the model on the test set\ntest_accuracy = rf_classifier.score(X_test, y_test)\nprint(\"Test set accuracy: {:.4f}\".format(test_accuracy))\n\nTraining set accuracy: 1.0000\nTest set accuracy: 0.9580\n\n\nWe can see that the accuracy of the random forest is 96%, which is better than the results obtained from individual decision trees even after pruning. It’s possible to fine-tune the max_features parameter or perform pre-pruning on the trees within the forest. However, in many cases, the default parameters are good enough to produce satisfactory results.\nSimilarly, we can visulize the importance of features by order:\n\nplot_feature_importances_cancer(rf_classifier)\n\n\n\n\nUnlike a single decision tree, many of the feature importances are great than 0 in a random forest. The weights of the features are more averaged out compared to those in a single decision tree.\nThe advantages of a random forest include:\n- Improved Generalization: Random Forest mitigates overfitting by aggregating predictions from multiple trees, leading to better generalization on new data.\n- Robustness: Random Forest is less sensitive to noise and outliers in the data compared to individual decision trees.\n- Feature Importance: Random Forest provides a measure of feature importance, helping in feature selection.\nThe disadvantages of a random forest include:\n- Less Interpretability: While an individual decision tree is easy to interpret, the ensemble nature of Random Forest makes it less straightforward to explain the overall model reasoning.\n- Computational Complexity: Training a Random Forest can be computationally expensive, especially with a large number of trees and features."
  },
  {
    "objectID": "posts/linear_nonlinear_regression/index.html",
    "href": "posts/linear_nonlinear_regression/index.html",
    "title": "Introduction to Linear Regression and Non-Linear Regression",
    "section": "",
    "text": "Regression is a statistical method used for modeling the relationship between a dependent variable (target) and one or more independent variables (features). The primary goal of regression analysis is to understand how the independent variables contribute to the variability in the dependent variable. The dependent variable in a regression task is numerical and usually continuous, rather than catergorical in a classification task. Regression is usually categorized into linear regression and non-linear regression.\nLinear Regression assumes a linear relationship between the dependent and independent variables, and models the relationship by fitting a linear equation to the observed data. The simplest linear equation takes the following form:\n\\(Y = mX + b\\)\nwhere:\n\n\\(Y\\) is the dependent variable.\n\\(X\\) is the independent variable.\n\\(m\\) is the slope of the line.\n\\(b\\) is the y-intercept.\n\nThe linear equation can be extended to include multiple independent variables as follows:\n\\(Y = b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n\\)\nThis is a multivariate linear regression assuming linear relationship in a higher dimension feature space.\nWe can make up a scatter plot with a fitting line to see what a typical linear regression looks like:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Generate synthetic data\nX = np.linspace(0, 10, 100)  # Independent variable\nnoise = np.random.normal(0, 2, 100)  # Gaussian noise\ny = 2 * X + 1 + noise  # Linear relationship with noise\n\n# Create a DataFrame\ndata = pd.DataFrame({'X': X, 'y': y})\n\nplt.figure(figsize=(10, 6))\nsns.regplot(x='X', y='y', data=data, line_kws={'color': 'red', 'label': 'Regression Line'})\n\nplt.title('Single-Variable Linear Regression')\nplt.xlabel('Independent Variable (X)')\nplt.ylabel('Dependent Variable (y)')\nplt.legend()\nplt.show()\n\n\n\n\nThe real-word data are usually more complicated, and can not be described by a simple linear equation. The non-Linear regression extends regression analysis to model relationships that are not linear. The functional form of the relationship between variables is more complex and can take various shapes. It is suitable for data where the variance of errors is not constant across all levels of the independent variable.\nNon-linear regression can involve various mathematical functions like exponential, logarithmic, polynomial, or even custom functions based on the nature of the data. This is called a parametric approach in which you use one or more mathematical models to fit the data.\nPolynomial equation is a typical non-linear regression equation and takes the following form:\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\\)\nwhere:\n\n\\(Y\\) is the dependent variable.\n\\(X\\) is the independent variable.\n\\(\\beta_0, \\beta_1, \\beta_2\\) are parameters.\n\\(\\epsilon\\) is the error term.\n\nLet’s see what this may look like:\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\n# Fit polynomial features\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n\n# Fit linear regression\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\n\n# Plot the data and the fitted curve\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=X.flatten(), y=y.flatten(), label='Data')\nsns.lineplot(x=X.flatten(), y=lin_reg.predict(X_poly).flatten(), color='red', label='Polynomial Regression')\nplt.title('Polynomial Regression Example')\nplt.xlabel('Independent Variable (X)')\nplt.ylabel('Dependent Variable (y)')\nplt.legend()\nplt.show()\n\n\n\n\nThere is another way to do non-linear regression. For example, regression with decision tree does not assume a mathematical relationship. Instead, the alrogithm recursively splits the data into subsets based on the values of input features, creating a tree structure. Predictions are made by averaging the target values in the leaf nodes reached by a given input. This is called a non-parametric approach. It is well-suited for capturing complex, non-linear relationships as it can approximate arbitrary shapes.\nSimilar to decision tree classifier, a decision tree regressor can be prone to overfitting, creating branches tailored to the training data. Pruning techniques are often applied to prevent overfitting. In addition, ensemble methods like Random Forests (bagging) and Gradient Boosting Decision Tree (boosting) are commonly used to enhance performance and reduce overfitting.\nLet’s see how a decision tree work on a regression task:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=3)\nregr_2 = DecisionTreeRegressor(max_depth=6)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\nX_test = np.arange(0.0, 2.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n\n# Plot the results using Seaborn for styling\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=X.flatten(), y=y.flatten(), label=\"data\")\nsns.lineplot(x=X_test.flatten(), y=y_1.flatten(), label=\"max_depth=3\", color=\"red\", linewidth=2)\nsns.lineplot(x=X_test.flatten(), y=y_2.flatten(), label=\"max_depth=6\", color=\"yellowgreen\", linewidth=2)\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.title(\"Decision Tree Regression Example\")\nplt.legend()\nplt.show()\n\n\n\n\nWe can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit.\nThe choice between Polynomial Regression and Decision Tree Regression depends on the specific characteristics of the dataset and the nature of the relationship being modeled. Polynomial regression is suitable when the underlying relationship is polynomial, while decision tree regression provides flexibility for capturing non-linear and complex patterns without specifying a particular function form. The decision often involves trade-offs between interpretability, computational efficiency, and the ability to handle different types of relationships."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Introduction to Clustering with DBSCAN",
    "section": "",
    "text": "Traditioal K-means clustering is well-suited for certain types of datasets and scenarios. It tends to perform well on datasets in which the clusters have a roughly circular shape in the feature space, similar variances, and approximately equal sizes. However, since K-means assumes that clusters are spherical and equally sized, there are a lot of situations that K-means cannot handle. For example, if the clusters have irregular shapes, varying densities, or the clusters are not convex, K-means will have poor performance on the datasets. As a result, alternative clustering algorithms may be considered. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a great candicate to handle such complex datasets.\nDBSCAN is a clustering algorithm designed to identify clusters in a dataset based on the density of data points. Unlike K-means, which assumes that clusters are spherical and equally sized, DBSCAN can discover clusters with irregular shapes and handle varying cluster densities.\nThe key idea behind DBSCAN is to group together data points that are close to each other in high-density regions and separate regions of lower density. It works based on the following parameters:\n\nEpsilon (eps)\nMinimum Points (min_samples)\n\nEpsilon defines the radius around a data point within which the algorithm looks for neighboring points. It is a key parameter that influences the size of the neighborhood considered for density estimation.\nMinimum Points specifies the minimum number of data points required to form a dense region. If a point has at least MinPts points within its epsilon neighborhood, it is considered a core point. Points that have fewer neighbors than MinPts but fall within the epsilon neighborhood of a core point are considered border points, while those with no neighbors are treated as noise or outliers.\nThe DBSCAN algorithm proceeds as follows:\n\nIdentify core points, which are data points with at least MinPts neighbors within a radius of eps.\nForm clusters by connecting core points that are within each other’s epsilon neighborhood.\nExpand the clusters by adding border points to them. Border points are points that have fewer than MinPts neighbors but are within the epsilon neighborhood of a core point.\nIdentify noise points as data points that do not belong to any cluster.\n\nThe following example shows how DBSCAN can perform better on a non-convex dataset than K-means, and how we can adjust eps and min_samples parameters to achieve the goal of clustering. First, let’s generate a synthetic data set with 3 clusters, 2 of which are non-convex.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\n\n# Generate synthetic datasets\nX1, y1 = datasets.make_circles(n_samples=5000, factor=.6, noise=.05)\nX2, y2 = datasets.make_blobs(n_samples=1000, n_features=2, centers=[[1.2, 1.2]], cluster_std=[[.1]], random_state=42)\nX = np.concatenate((X1, X2))\n\n# Create a scatter plot\nsns.set(style=\"white\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X[:, 0], y=X[:, 1], marker='o', alpha=0.7)\n\nplt.show()\n\n\n\n\nLet’s see how K-means work on the dataset. The n_clusters is set to 3 knowing that there are 3 clusters in the dataset.\n\nfrom sklearn.cluster import KMeans\n\n# Perform KMeans clustering\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\ny_pred = kmeans.fit_predict(X)\n\n# Create a scatter plot\nsns.set(style=\"white\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred, palette='viridis', legend=False, marker='o', alpha=0.7)\n\nplt.show();\n\n\n\n\nApparently, K-means can not identify the two non-convex clusters. Let’s set up a DBSCAN to see how it performs.\n\nfrom sklearn.cluster import DBSCAN\n\n# Perform DBSCAN clustering\ndbscan = DBSCAN(eps=0.2, min_samples=5)\ny_pred = dbscan.fit_predict(X)\n\n# Create a scatter plot\nsns.set(style=\"white\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred, palette='viridis', legend=False, marker='o', alpha=0.7)\n\nplt.show()\n\n\n\n\nThe above DBSCAN algorithm sets the eps at 0.2 and min_samples at 0.5. It can differentiate between the convex cluster and the non-convex clusters, which a sign of improvement. However, it is not differentiating the two non-convex clusters, which could mean that the size of the dense region may be too large. We can either decrease the eps or min_samples to shrink the dense region.\n\n# Update hyperparameters for DBSCAN clustering\ndbscan = DBSCAN(eps=0.1, min_samples=5)\ny_pred = dbscan.fit_predict(X)\n\n# Create a scatter plot\nsns.set(style=\"white\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_pred, palette='viridis', legend=False, marker='o', alpha=0.7)\n\nplt.show()\n\n\n\n\nBy decreasing the eps to 0.1, the DBSCAN alrogithm now does an excellent job in differentiating the two non-convex clusters.\nThe example demonstrate DBSCAN’s ability to handle clusters of arbitrary shapes, insensitivity to the order of the input data, and ability to discover clusters of different shapes and sizes. Additionally, DBSCAN can automatically determine the number of clusters in the dataset. However, DBSCAN’s performance can be sensitive to the choice of parameters, and it may struggle with datasets of varying densities. Tuning these parameters appropriately for the specific dataset is crucial for obtaining meaningful clustering results."
  }
]